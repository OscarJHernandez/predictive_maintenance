{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  savgol_eng_1/  already exists\n",
      "[0.70214949 0.20322635]\n",
      "         pc1        pc2\n",
      "0  10.332965  14.367665\n",
      "1   7.594580  13.597641\n",
      "2   6.240766   6.366140\n",
      "3   5.791491   4.843604\n",
      "4   5.691647   1.887474\n",
      "Directory  savgol_eng_2/  already exists\n",
      "[0.91998902 0.04108382]\n",
      "        pc1       pc2\n",
      "0 -8.676931  2.090571\n",
      "1 -7.337142  0.936188\n",
      "2 -5.931727 -0.200613\n",
      "3 -6.124439 -0.098380\n",
      "4 -5.508644 -0.769333\n",
      "Directory  savgol_eng_3/  already exists\n",
      "[0.81141236 0.08329364]\n",
      "        pc1       pc2\n",
      "0 -7.542262  7.968435\n",
      "1 -4.213060  5.975667\n",
      "2 -4.518401  3.873021\n",
      "3 -3.923304  4.624723\n",
      "4 -4.024247  2.469918\n",
      "Directory  savgol_eng_4/  Created \n",
      "[0.7953519  0.08701705]\n",
      "         pc1       pc2\n",
      "0  10.379894  8.826812\n",
      "1  10.271483  7.252146\n",
      "2   7.533436  2.833865\n",
      "3   6.850760  2.784960\n",
      "4   4.656471 -0.451445\n",
      "Directory  savgol_eng_5/  Created \n",
      "[0.92411654 0.04727792]\n",
      "        pc1       pc2\n",
      "0 -9.584020  2.968332\n",
      "1 -5.855212  0.088399\n",
      "2 -5.887191  0.443084\n",
      "3 -5.814912  0.039007\n",
      "4 -6.063504  0.256074\n",
      "Directory  savgol_eng_6/  Created \n",
      "[0.7807627  0.11533218]\n",
      "        pc1        pc2\n",
      "0 -7.784151  15.429765\n",
      "1 -4.709475  -1.479095\n",
      "2 -5.698845  -0.425927\n",
      "3 -5.327813  -0.892355\n",
      "4 -5.293687   0.140850\n",
      "Directory  savgol_eng_7/  Created \n",
      "[0.714182   0.12763249]\n",
      "         pc1        pc2\n",
      "0  12.320641  18.776388\n",
      "1   4.930689   7.892866\n",
      "2   4.718101   2.531605\n",
      "3   4.759627   1.535764\n",
      "4   5.208813   1.561001\n",
      "emcee: Exception while calling your likelihood function:\n",
      "  params: [ 6.29926315e-02 -3.35216350e-04 -2.41018238e+00  1.46164221e+00]\n",
      "  args: (range(185, 205), array([-1.89385924, -1.913736  , -1.92706666, -1.9584702 , -1.99156431,\n",
      "       -2.02265294, -2.06161206, -2.10643953, -2.1574496 , -2.21396479,\n",
      "       -2.25590019, -2.31340066, -2.38263667, -2.4626558 , -2.5515266 ,\n",
      "       -2.64668475, -2.74615253, -2.85037939, -2.9547103 , -3.0553093 ]))\n",
      "  kwargs: {}\n",
      "  exception:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/anaconda3/envs/pm_2019/lib/python3.5/site-packages/emcee/ensemble.py\", line 519, in __call__\n",
      "    return self.f(x, *self.args, **self.kwargs)\n",
      "  File \"<ipython-input-1-284c0c8eee06>\", line 285, in lnprob\n",
      "    return lnlike(theta, x, y)\n",
      "  File \"<ipython-input-1-284c0c8eee06>\", line 277, in lnlike\n",
      "    model = a1*x +a2*x*x+ b\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-284c0c8eee06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0my_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mbayesian_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_N\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0ma1_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-284c0c8eee06>\u001b[0m in \u001b[0;36mbayesian_fit\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memcee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnsembleSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnwalkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_mcmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pm_2019/lib/python3.5/site-packages/emcee/sampler.py\u001b[0m in \u001b[0;36mrun_mcmc\u001b[0;34m(self, pos0, N, rstate0, lnprob0, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         for results in self.sample(pos0, lnprob0, rstate0, iterations=N,\n\u001b[0;32m--> 172\u001b[0;31m                                    **kwargs):\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pm_2019/lib/python3.5/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, p0, lnprob0, rstate0, blobs0, iterations, thin, storechain, mh_proposal)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mS0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     q, newlnp, acc, blob = self._propose_stretch(p[S0], p[S1],\n\u001b[0;32m--> 259\u001b[0;31m                                                                  lnprob[S0])\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                         \u001b[0;31m# Update the positions, log probabilities and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pm_2019/lib/python3.5/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36m_propose_stretch\u001b[0;34m(self, p0, p1, lnprob0)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Calculate the proposed positions and the log-probability there.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mzz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mnewlnprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lnprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Decide whether or not the proposals should be accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pm_2019/lib/python3.5/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36m_get_lnprob\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# Run the log-probability calculations (optionally in parallel).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnprobfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pm_2019/lib/python3.5/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-284c0c8eee06>\u001b[0m in \u001b[0;36mlnprob\u001b[0;34m(theta, x, y)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m#return lp + lnlike(theta, x, y, yerr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlnlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-284c0c8eee06>\u001b[0m in \u001b[0;36mlnlike\u001b[0;34m(theta, x, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlnlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0minv_sigma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minv_sigma2\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_sigma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import cluster\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for eng_id in range(1,101):\n",
    "    \n",
    "\n",
    "    # Read in all of the input\n",
    "    in_data = 'train_FD001.txt'\n",
    "    #eng_id = 30\n",
    "    N_clusters =3 \n",
    "    N_steps = 40 # Number of steps to do the regression analysis\n",
    "    savgol_window_size = 21\n",
    "    out_data = 'savgol_eng_'+str(eng_id)+\"/\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(out_data)\n",
    "        print(\"Directory \" , out_data ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , out_data,  \" already exists\")\n",
    "\n",
    "\n",
    "    data = pd.read_csv('data/'+in_data,header=None,delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    settings = ['operational_setting_1','operational_setting_2','operational_setting_3']\n",
    "    sensors = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8',\n",
    "               'sensor_9','sensor_10', 'sensor_11','sensor_12', 'sensor_13','sensor_14', 'sensor_15','sensor_16',\n",
    "               'sensor_17','sensor_18', 'sensor_19','sensor_20', 'sensor_21']\n",
    "\n",
    "    cols = ['engine_num','time_cycles']+settings+sensors\n",
    "    data.columns = cols\n",
    "\n",
    "\n",
    "    sensor_data = data.drop(settings,axis=1)\n",
    "    sensor_data = sensor_data[sensor_data['engine_num']==eng_id]\n",
    "    sensor_data = sensor_data[sensors]\n",
    "\n",
    "\n",
    "    # Now we examine the correlations \n",
    "    eng1_data = sensor_data\n",
    "\n",
    "    # These three sensors are flat lines\n",
    "    eng1_data = eng1_data.drop([\"sensor_1\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_18\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_19\"], axis=1)\n",
    "\n",
    "    corr = eng1_data.corr()\n",
    "    corr = np.abs(corr)\n",
    "\n",
    "    # plot the heatmap\n",
    "    sns.heatmap(corr, \n",
    "            xticklabels=corr.columns,\n",
    "            yticklabels=corr.columns)\n",
    "    plt.title(\"Engine Number: \" + str(eng_id))\n",
    "    plt.savefig(out_data+\"corr_data_full_\"+in_data+\"_sensor_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    # Now we examine the correlations \n",
    "    eng1_data = sensor_data\n",
    "\n",
    "    # These three sensors are flat lines\n",
    "    eng1_data = eng1_data.drop([\"sensor_1\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_18\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_19\"], axis=1)\n",
    "\n",
    "    # Drop these correlated sensors\n",
    "    eng1_data = eng1_data.drop([\"sensor_5\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_6\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_10\"], axis=1)\n",
    "    eng1_data = eng1_data.drop([\"sensor_16\"], axis=1)\n",
    "    corr = np.abs(eng1_data.corr())\n",
    "\n",
    "    # plot the heatmap\n",
    "    plt.clf()\n",
    "    sns.heatmap(corr, \n",
    "            xticklabels=corr.columns,\n",
    "            yticklabels=corr.columns)\n",
    "    plt.title(\"Engine Number: \" + str(eng_id))\n",
    "    plt.savefig(out_data+\"corr_data_sub_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    #================================================================\n",
    "    # Choose the N-sensors to examine\n",
    "\n",
    "    N_ind_sensors_label = []\n",
    "    N_ind_sensors_name = [] \n",
    "\n",
    "    corr = np.abs(eng1_data.corr())\n",
    "    M = np.asarray(corr.iloc[:,:])\n",
    "    Z = linkage(M,'single' )\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    labelsize=20\n",
    "    ticksize=15\n",
    "    plt.title('Hierarchical Clustering Dendrogram for Sensor Data', fontsize=labelsize)\n",
    "    plt.xlabel('stock', fontsize=labelsize)\n",
    "    plt.ylabel('distance', fontsize=labelsize)\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=8.,  # font size for the x axis labels\n",
    "        labels = corr.columns\n",
    "    )\n",
    "    plt.yticks(fontsize=ticksize)\n",
    "    plt.xticks(rotation=-90, fontsize=ticksize)\n",
    "    plt.savefig(out_data+\"sensor_dendrogram_sub_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    # Lets generate three clusters based on the data\n",
    "    agglo = cluster.FeatureAgglomeration(n_clusters=N_clusters)\n",
    "    agglo.fit(M)\n",
    "    M_reduced = agglo.transform(M)\n",
    "\n",
    "    cluster_label = agglo.labels_\n",
    "    data_col = corr.columns\n",
    "\n",
    "    # Initialize our array\n",
    "    N_ind_sensors_label.append(cluster_label[0])\n",
    "    N_ind_sensors_name.append(data_col[0])\n",
    "\n",
    "    for k in range(1,len(cluster_label)):\n",
    "\n",
    "        if(cluster_label[k] not in N_ind_sensors_label):\n",
    "            N_ind_sensors_label.append(cluster_label[k])\n",
    "            N_ind_sensors_name.append(data_col[k])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rms(y):\n",
    "\n",
    "        s = np.dot(y,y)\n",
    "        s = s/float(len(y))\n",
    "        s =np.sqrt(s)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def max_peak(y):\n",
    "\n",
    "        s = np.max(y)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def line_int(y):\n",
    "\n",
    "        s = 0.0\n",
    "\n",
    "        for i in range(1,len(y)):\n",
    "            s+= np.abs(y[i]-y[i-1])\n",
    "\n",
    "        return s\n",
    "\n",
    "    def energy(y):\n",
    "\n",
    "        y = y -np.mean(y)\n",
    "        s = np.dot(y,y)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def std(y):\n",
    "\n",
    "        s = np.std(y)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def compute_property(func,y_vec):\n",
    "\n",
    "        N = len(y_vec)\n",
    "        y_func = np.zeros(N)\n",
    "\n",
    "        for i in range(1,N+1):\n",
    "            yi = y_vec[0:i]\n",
    "            fi = func(yi)\n",
    "            y_func[i-1] = fi\n",
    "\n",
    "\n",
    "\n",
    "        return y_func\n",
    "\n",
    "\n",
    "    #====================================================================================\n",
    "    # Standard Scaler\n",
    "    #====================================================================================\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    eng1_data_ind = sensor_data[N_ind_sensors_name]\n",
    "\n",
    "    # Generate a new data frame with all of these new features\n",
    "    X =  pd.DataFrame()\n",
    "\n",
    "    energy_set = ['energy_'+str(int(k)) for k in range(0,N_clusters)]\n",
    "    rms_set = ['rms_'+str(int(k)) for k in range(0,N_clusters)]\n",
    "    line_set = ['line_'+str(int(k)) for k in range(0,N_clusters)]\n",
    "    max_set = ['max_'+str(int(k)) for k in range(0,N_clusters)]\n",
    "    std_set = ['std_'+str(int(k)) for k in range(0,N_clusters)]\n",
    "\n",
    "    for k in range(len(energy_set)):\n",
    "        feature_name_1 = energy_set[k]\n",
    "        feature_name_2 = rms_set[k]\n",
    "        feature_name_3 = line_set[k]\n",
    "        feature_name_4 = max_set[k]\n",
    "        feature_name_5 = std_set[k]\n",
    "        X[feature_name_1] = compute_property(energy, eng1_data_ind.iloc[0:,k].values)\n",
    "        X[feature_name_2] = compute_property(rms, eng1_data_ind.iloc[0:,k].values)\n",
    "        X[feature_name_3] = compute_property(line_int, eng1_data_ind.iloc[0:,k].values)\n",
    "        X[feature_name_4] = compute_property(max_peak, eng1_data_ind.iloc[0:,k].values)\n",
    "        X[feature_name_5] = compute_property(std, eng1_data_ind.iloc[0:,k].values)\n",
    "\n",
    "\n",
    "    all_features = energy_set+rms_set+line_set+max_set+std_set\n",
    "\n",
    "    X = X.loc[:, all_features].values\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "    #====================================================================================\n",
    "    # PCA Analysis\n",
    "    #====================================================================================\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "                 , columns = ['pc1', 'pc2'])\n",
    "\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(principalDf.head())\n",
    "\n",
    "    V1 = principalDf['pc1']\n",
    "    V2 = principalDf['pc2']\n",
    "\n",
    "    plt.clf()\n",
    "    plt.title(\"PCA 1\")\n",
    "    plt.plot(V1)\n",
    "    plt.xlabel(\"Cycles\", size=20)\n",
    "    plt.ylabel(\"PCA [unitless]\", size=20)\n",
    "    plt.savefig(out_data+\"PCA1_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    #====================================================================================\n",
    "    # Bayesian Fit\n",
    "    #====================================================================================\n",
    "    import emcee\n",
    "    import scipy.optimize as op\n",
    "    from scipy.signal import savgol_filter\n",
    "\n",
    "    x = range(len(V1))\n",
    "    y1 = savgol_filter(V1,savgol_window_size,3)\n",
    "    y2 = savgol_filter(V2,savgol_window_size,3)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.title(\"PCA 1 Savgol Filter\")\n",
    "    plt.plot(x,y1)\n",
    "    plt.xlabel(\"Cycles\", size=20)\n",
    "    plt.ylabel(\"PCA [unitless]\", size=20)\n",
    "    plt.savefig(out_data+\"PCA1_filter_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "    def bayesian_fit(x,y): \n",
    "\n",
    "        def lnlike(theta, x, y):\n",
    "            a1, a2,b,sigma = theta\n",
    "            model = a1*x +a2*x*x+ b\n",
    "            inv_sigma2 = 1.0/sigma**2\n",
    "            return -0.5*(np.sum((y-model)**2*inv_sigma2- np.log(inv_sigma2)))\n",
    "\n",
    "        def lnprob(theta, x, y):\n",
    "            #lp = lnprior(theta)\n",
    "\n",
    "            #return lp + lnlike(theta, x, y, yerr)\n",
    "            return lnlike(theta, x, y)\n",
    "\n",
    "\n",
    "        nll = lambda *args: -lnlike(*args)\n",
    "        result = op.minimize(nll, [1.0,1.0,1.0,1.0], args=(x, y))\n",
    "        a1_ml, a2_ml, b_ml, sigma_ml = result[\"x\"]\n",
    "\n",
    "        ndim, nwalkers = 4, 8\n",
    "        pos = [result[\"x\"] + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y))\n",
    "\n",
    "        sampler.run_mcmc(pos, 100)\n",
    "\n",
    "        samples = sampler.chain[:, 50:, :].reshape((-1, ndim))\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "    Nk = []\n",
    "    a1_k = []\n",
    "    a2_k = []\n",
    "    a2_error_k = []\n",
    "    a1_error_k = []\n",
    "\n",
    "    n_min = 20\n",
    "    n_max = len(y1)\n",
    "    dh = int(float(n_max-n_min)/float(N_steps))\n",
    "\n",
    "    for N in range(n_min,n_max,dh):\n",
    "\n",
    "        x_N = x[N-n_min:N]\n",
    "        y_N = y1[N-n_min:N]\n",
    "\n",
    "        samples =  bayesian_fit(x_N,y_N)\n",
    "\n",
    "        a1_N = np.mean(samples[:,0])\n",
    "        a2_N = np.mean(samples[:,1])\n",
    "\n",
    "        a2_k.append(a2_N)\n",
    "        a1_k.append(a1_N)\n",
    "\n",
    "        a1_error_k = np.std(samples[:,0])\n",
    "        a2_error_k = np.std(samples[:,1])\n",
    "        Nk.append(N)\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(Nk,a1_k,'-o',color='green')\n",
    "    plt.title(\"Linear Coeff vs Cycle\",size=20)\n",
    "    plt.xlabel(\"Cycles\", size=20)\n",
    "    plt.ylabel(\"a1\", size=20)\n",
    "    plt.fill_between(Nk, a1_k-a1_error_k, a1_k+a1_error_k,alpha=0.2,color='green')\n",
    "    plt.savefig(out_data+\"a1_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(Nk,a2_k,'-o',color='blue')\n",
    "    plt.title(\"Quadratic Coeff vs Cycle\",size=20)\n",
    "    plt.xlabel(\"Cycles\", size=20)\n",
    "    plt.ylabel(\"a2\", size=20)\n",
    "    plt.fill_between(Nk, a2_k-a2_error_k, a2_k+a2_error_k,alpha=0.2,color='blue')\n",
    "    plt.savefig(out_data+\"a2_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "\n",
    "    # Now we determine the range of the anomalies\n",
    "    from scipy.signal import argrelextrema\n",
    "    from scipy.signal import chirp, find_peaks, peak_widths\n",
    "\n",
    "    N_burn = 10\n",
    "\n",
    "    Nk_burn = Nk[N_burn:]\n",
    "    a1_burn_k = np.abs(a1_k[N_burn:])\n",
    "    a2_burn_k = np.abs(a2_k[N_burn:])\n",
    "\n",
    "    anom_a1 = np.argmax(np.abs(a1_burn_k))\n",
    "    anom_a2 = np.argmax(np.abs(a2_burn_k))\n",
    "\n",
    "    peaks_a1, _ = find_peaks(np.asarray(a1_burn_k))\n",
    "    results_half_a1 = peak_widths(np.asarray(a1_burn_k), peaks_a1, rel_height=0.5)\n",
    "\n",
    "    peaks_a2, _ = find_peaks(np.asarray(a2_burn_k))\n",
    "    results_half_a2 = peak_widths(np.asarray(a2_burn_k), peaks_a2, rel_height=0.5)\n",
    "\n",
    "\n",
    "    anom_a1 = peaks_a1[np.argmax(a1_burn_k[peaks_a1]/results_half_a1[0])]\n",
    "    anom_a2 = peaks_a2[np.argmax(a2_burn_k[peaks_a2]/results_half_a2[0])]\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(Nk_burn,np.abs(a1_burn_k),'-o',color='green')\n",
    "    [plt.axvline(Nk_burn[peaks_a1[k]], linewidth=1, color='g') for k in range(len(peaks_a1))]\n",
    "    plt.axvline(Nk_burn[anom_a1], linewidth=2, color='purple') \n",
    "    plt.title(\"Linear Coeff vs Cycle\",size=20)\n",
    "    plt.xlabel(\"Cycles\", size=20)\n",
    "    plt.ylabel(\"a1\", size=20)   \n",
    "    plt.savefig(out_data+\"a1_peaks_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(Nk_burn,np.abs(a2_burn_k),'-o',color='blue')\n",
    "    [plt.axvline(Nk_burn[peaks_a2[k]], linewidth=1, color='b') for k in range(len(peaks_a2))]\n",
    "    plt.axvline(Nk_burn[anom_a2], linewidth=2, color='purple') \n",
    "    plt.title(\"Quadratic Coeff vs Cycle\",size=20)\n",
    "    plt.xlabel(\"Cycles\", size=20)\n",
    "    plt.ylabel(\"a2\", size=20)\n",
    "    plt.savefig(out_data+\"a2_peaks_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n",
    "\n",
    "\n",
    "    for k in range(0,len(N_ind_sensors_name)):\n",
    "        plt.clf()\n",
    "        plt.title(N_ind_sensors_name[k],size='20')\n",
    "        plt.plot(eng1_data_ind.iloc[:,k].values)\n",
    "\n",
    "        plt.axvspan(Nk_burn[anom_a1]-n_min,Nk_burn[anom_a1],color='purple',alpha=0.3)\n",
    "        plt.axvline(Nk_burn[anom_a1]-n_min,color='purple')\n",
    "        plt.axvline(Nk_burn[anom_a1],color='purple',label='Linear Anomaly')\n",
    "\n",
    "        plt.axvspan(Nk_burn[anom_a2]-n_min,Nk_burn[anom_a2],color='red',alpha=0.3)\n",
    "        plt.axvline(Nk_burn[anom_a2],color='red')\n",
    "        plt.axvline(Nk_burn[anom_a2]-n_min,color='red',label=\"Quadratic Anomaly\")\n",
    "\n",
    "        plt.xlabel(\"Cycles\",size=20)\n",
    "        plt.legend()\n",
    "        plt.savefig(out_data+\"sensor_\"+str(N_ind_sensors_name[k])+\"_anomaly_\"+in_data+\"_eng_\"+str(int(eng_id))+'.pdf',bboxes='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pm_2019]",
   "language": "python",
   "name": "conda-env-pm_2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
